{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9173ef",
   "metadata": {},
   "source": [
    "# Assembly\n",
    "\n",
    "This notebook will go through the workflow using the metaspades and megahit tools. These tools produce contigs (a series of DNA sequences)\n",
    "\n",
    "1. An introduction to [Metaspades](https://cab.spbu.ru/files/release3.12.0/manual.html)\n",
    "2. An introduction to [Megahit](https://github.com/voutcn/megahit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81daedb",
   "metadata": {},
   "source": [
    "## Metaspades\n",
    "\n",
    "In this section we are going to assemble our reads into contigs. Contigs are fragments of DNA that reprepresent part of a genome. If you are lucky, you might even be able to assemble an entire genome in a single contig! But, most of the time, contigs are just part of a genome with missing fragments inbetween contigs that prevent you from assembling the entire genome.  \n",
    "\n",
    "You will be assembling your reads using a program called spades, that has a metaspades.py program that is container within it for assembling metagenome comprised of multiple organisms.\n",
    "\n",
    "It's good to note that this assembler is memory intensive, and for large files it takes a lot of resources and time. A common error on large files are running out of memory to complete the job in the HPC. The HPCs have different allotment of resources and we can modify our script if it requires more memory. \n",
    "\n",
    "Puma can have 94 CPUs @ 5gb/CPU <br>\n",
    "Ocelote can have 28 CPUs @ 6gb/CPU\n",
    "\n",
    "This [HPC documentation](https://public.confluence.arizona.edu/display/UAHPC/Running+Jobs+with+SLURM) is handy to have as you edit your scripts and use different HPCs within UA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4758e2",
   "metadata": {},
   "source": [
    "### Data Management\n",
    "\n",
    "We'll be creating two assemblies based on the trim reads we gathered from running trimmomatic. Let's setup the output directories ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97114073",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /xdisk/bhurwitz/YOUR_NAME/assembly/out_spades\n",
    "!mksir /xdisk/bhurwitz/YOUR_NAME/assembly/out_megahit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nano run_metaspades.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a3b7b",
   "metadata": {},
   "source": [
    "And you will need to paste in & modify the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ac65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=run_metaspades\n",
    "#SBATCH --account=bhurwitz\n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --time=24:00:00\n",
    "#SBATCH --cpus-per-task=28\n",
    "#SBATCH --mem-per-cpu=5gb\n",
    "#SBATCH --array=0-46\n",
    "\n",
    "#get array based of SRR List\n",
    "names=($(cat /xdisk/bhurwitz/YOUR_NAME/bio_pipeline/data/SRR_Acc_List.txt))\n",
    "\n",
    "readdir = \"/xdisk/bhurwitz/YOUR_NAME/trimming/trimmed_reads\"\n",
    "assemdir=\"/xdisk/bhurwitz/YOUR_NANE/assembly/out_spades\"\n",
    "\n",
    "#add threads flag & exposition on adding threads or it runs inefficient\n",
    "apptainer run /contrib/singularity/shared/bhurwitz/spades:3.15.5--h95f258a_1.sif metaspades.py \\\n",
    "   -o ${assemdir}/${names[${SLURM_ARRAY_TASK_ID}]} \\\n",
    "   --pe1-1 ${readdir}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz \\\n",
    "   --pe1-2 ${readdir}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa37d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch ~/run_metaspades.sh"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b20074a",
   "metadata": {},
   "source": [
    "## Megahit\n",
    "\n",
    "We'll now repeat the process using megahit -- A different algorithm to assemble your contigs. This assembler works a lot faster, using less resources but isn't as accurate as spades. If you find spades crashing due to memory-out errors megahit will be able to assemble the bigger read files. \n",
    "\n",
    "Other options we'll explore in later notebooks are removing reads based on a reference genome database. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nano run_megahit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=run_megahit\n",
    "#SBATCH --account=bhurwitz\n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --time=24:00:00\n",
    "#SBATCH --cpus-per-task=28\n",
    "#SBATCH --mem-per-cpu=5gb\n",
    "#SBATCH --array=0-46\n",
    "\n",
    "\n",
    "#get array based of SRR List\n",
    "names=($(cat /xdisk/bhurwitz/YOUR_NAME/bio_pipeline/data/SRR_Acc_List.txt))\n",
    "\n",
    "readdir = \"/xdisk/bhurwitz/YOUR_NAME/trimming/trimmed_reads\"\n",
    "assemdir=\"/xdisk/bhurwitz/YOUR_NANE/assembly/out_megahit\"\n",
    "\n",
    "#double check if megahit needs a thread command\n",
    "apptainer run /contrib/singularity/shared/bhurwitz/megahit:1.2.9--h5b5514e_3.sif megahit \\\n",
    "   -1 ${readdir}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz /\n",
    "   -2 ${readdir}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz /\n",
    "   -o ${assemdir}/${names[${SLURM_ARRAY_TASK_ID}]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
