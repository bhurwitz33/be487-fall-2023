{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de7bac8",
   "metadata": {},
   "source": [
    "# Taxonomic Annotation of Reads\n",
    "\n",
    "This notebook will go through the workflow assigning taxonomy to reads from a microbiome\n",
    "\n",
    "1. Taxonomic assignment of reads using Kraken2\n",
    "2. Refinement of the taxonomic annotation using Bracken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebed36",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "You will need to rerun this section each time you come back to this notebook to reset all directories and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables for your netid and xfile\n",
    "netid = \"MY_NETID\"\n",
    "xfile = \"MY_XFILE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d388ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into the working directory\n",
    "work_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/11_taxonomy\"\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dffed",
   "metadata": {},
   "source": [
    "## Creating a config file\n",
    "Let's create a config file with all of the variables we will need in the scripts below. Then when we want to use these variables in the script, we will \"source\" the config file to set the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config file with all of the variables you need\n",
    "!echo \"export NETID=$netid\" > config.sh\n",
    "!echo \"export XFILE=$xfile\" >> config.sh\n",
    "!echo \"export WORK_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/11_taxonomy\" >> config.sh\n",
    "!echo \"export XFILE_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/05_getting_data\" >> config.sh\n",
    "!echo \"export FASTQ_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/07_contam_removal\" >> config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the config file to be sure it is correct\n",
    "# Is your netid and xfile correct? Do you have the right directories?\n",
    "!cat config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeaabfe",
   "metadata": {},
   "source": [
    "## Step 1: Assigning taxonomy to the reads\n",
    "\n",
    "In this step, we will try to assign taxonomy to each of the reads in our microbiome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run bowtie2 to align reads to a human reference\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via the `source ./config.sh` command in the script.\n",
    "# 2. bowtie2 runs on each of the fastq files in the trimmed $FASTQ_DIR\n",
    "# 3. The results will be written into our $WORK_DIR\n",
    "# 4. Notice that we are asking for alot more resource (24 cores and 5G of memory per core), we are also asking for more time (24 hours)\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=24:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-4                         \n",
    "#SBATCH --output=Job-rem_human-%a.out\n",
    "#SBATCH --cpus-per-task=24\n",
    "#SBATCH --mem-per-cpu=5G                                    \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source $SLURM_SUBMIT_DIR/config.sh\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "\n",
    "SAMPLE_ID=${names[${SLURM_ARRAY_TASK_ID}]}\n",
    "\n",
    "PAIR1=${FASTQ_DIR}/${SAMPLE_ID}_1.fastq.gz\n",
    "PAIR2=${FASTQ_DIR}/${SAMPLE_ID}_2.fastq.gz\n",
    "\n",
    "### reads with human removed\n",
    "BOWTIE_NAME=\"${WORK_DIR}/${SAMPLE_ID}_%.fastq.gz\"\n",
    "SAM_NAME=\"${WORK_DIR}/${SAMPLE_ID}_human_removed.sam\"\n",
    "\n",
    "### reads mapped to human\n",
    "MET_NAME=\"${WORK_DIR}/${SAMPLE_ID}_hostmap.log\"\n",
    "\n",
    "apptainer run /contrib/singularity/shared/bhurwitz/bowtie2:2.5.1--py39h6fed5c7_2.sif bowtie2 \\\n",
    "    -p 24 -x $HUM_DB -1 $PAIR1 -2 $PAIR2 --un-conc-gz $BOWTIE_NAME 1> $SAM_NAME 2> $MET_NAME\n",
    "\n",
    "rm $SAM_NAME\n",
    "'''\n",
    "\n",
    "with open('remove_human_parallel.sh', mode='w') as file:\n",
    "    file.write(my_code)\n",
    "\n",
    "\n",
    "#!/bin/bash -l\n",
    "#SBATCH --job-name=kraken2_%a\n",
    "#SBATCH --output=errout/kraken2_output_%a.txt\n",
    "#SBATCH --error=errout/kraken2_error_%a.txt\n",
    "#SBATCH --time=24:00:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem-per-cpu=4G\n",
    "#SBATCH --ntasks=1\n",
    "\n",
    "# source configuration and copy to the result dir \n",
    "source $SLURM_SUBMIT_DIR/config/config_testing.sh\n",
    "RESULT_DIR=\"$OUT_DIR/out_kraken2\"\n",
    "mkdir $RESULT_DIR\n",
    "HUMAN_READ_DIR=\"$RESULT_DIR/human_reads\"\n",
    "NONHUMAN_READ_DIR=\"$RESULT_DIR/nonhuman_reads\"\n",
    "mkdir $HUMAN\n",
    "mkdir $NONHUMAN\n",
    "\n",
    "cp $SLURM_SUBMIT_DIR/config/config_testing.sh $RESULT_DIR\n",
    "\n",
    "# echo for log\n",
    "echo \"job started\"; hostname; date\n",
    "\n",
    "# go to folder\n",
    "cd $IN_DIR\n",
    "\n",
    "# get sample ID\n",
    "export SAMPLE_ID=`awk NR==$SLURM_ARRAY_TASK_ID $IN_LIST`\n",
    "\n",
    "#debugging\n",
    "echo sample list:  $SAMPLE_ID\n",
    "echo slurm_id: $SLURM_ARRAY_TASK_ID\n",
    "\n",
    "PAIR1=${SAMPLE_ID}_human_removed_1.fastq*\n",
    "PAIR2=${SAMPLE_ID}_human_removed_2.fastq*\n",
    "\n",
    "P1=\"$IN_DIR/$PAIR1\"\n",
    "P2=\"$IN_DIR/$PAIR2\"\n",
    "\n",
    "OUT=\"$RESULT_DIR/$SAMPLE_ID\"\n",
    "mkdir $OUT\n",
    "DB=\"$DB_DIR\"\n",
    "# DB=\"$DB_DIR/database${KMER_SIZE}mers.kmer_distrib\" TESTING TAXO USAGE\n",
    "\n",
    "apptainer run $KRAKEN2 kraken2 --db ${DB} --paired --classified-out $OUT/cseqs.fq --output $OUT/kraken_results.txt --report $OUT/kraken_report.txt --use-n\n",
    "ames --threads ${SLURM_CPUS_PER_TASK} $P1 $P2\n",
    "\n",
    "#apptainer run $KRAKEN2 kraken2 \\\n",
    "#              --db ${DB} \\ \n",
    "#              --paired \\\n",
    "#              --classified-out $OUT/cseqs.fq \\\n",
    "#              --output $OUT/kraken_results.txt \\\n",
    "#              --report $OUT/kraken_report.txt \\\n",
    "#              --use-names \\\n",
    "#              --threads ${SLURM_CPUS_PER_TASK} \\ \n",
    "#              $P1 $P2 \n",
    "\n",
    "# run krakentools to extract human/microbial reads\n",
    "conda activate kraken2\n",
    "# selects all reads from a given set of Kraken taxids (and all children)\n",
    "# select all reads from human (9606 for human)\n",
    "TAXID=9606\n",
    "$KRAKENTOOLS/extract_kraken_reads.py \\\n",
    "             -k $OUT/kraken_results.txt \\ \n",
    "             --report $OUT/kraken_report.txt \\\n",
    "             -s1 $P1 \\\n",
    "             -s2 $P2 \\\n",
    "             --taxid $TAXID \\  \n",
    "             --output $HUMAN_READ_DIR/r1.fq \\  \n",
    "             --output2 $HUMAN_READ_DIR/r2.fq \\\n",
    "             --include-children \\\n",
    "             --fastq-output\n",
    "\n",
    "gzip $HUMAN_READ_DIR/r1.fq\n",
    "gzip $HUMAN_READ_DIR/r2.fq\n",
    "\n",
    "# selects all reads NOT from a given set of Kraken taxids (and all children)\n",
    "# NOT human \n",
    "$KRAKENTOOLS/extract_kraken_reads.py \\\n",
    "             -k $OUT/kraken_results.txt \\\n",
    "             --report $OUT/kraken_report.txt \\\n",
    "             -s1 $P1 \\\n",
    "             -s2 $P2 \\\n",
    "             --taxid $TAXID \\\n",
    "             --output $NONHUMAN_READ_DIR/r1.fq \\\n",
    "             --output2 $NONHUMAN_READ_DIR/r2.fq \\\n",
    "             --include-children \\\n",
    "             --exclude \\\n",
    "             --fastq-output\n",
    "\n",
    "gzip $NONHUMAN_READ_DIR/r1.fq\n",
    "gzip $NONHUMAN_READ_DIR/r2.fq\n",
    " \n",
    "echo \"Finished `date`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the code and make sure your script above was created.\n",
    "!cat remove_human_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e42306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should be in your working directory when you run this script\n",
    "# do you see your config.sh file, and the remove_human_parallel.sh script?\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe884278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run sbatch to run bowtie on each of your trimmed fastq files to remove human\n",
    "# Remember that this may take a while to run, so take a break, and get a coffee.\n",
    "!sbatch ./remove_human_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "# Check for all jobs under your netid\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e70ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once your jobs have run (or are running) you can check the progress\n",
    "# and also look for errors in the *out files\n",
    "# For example, you can look at Job-rem_human-0.out\n",
    "!ls\n",
    "!cat Job-rem_human-0.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that all of your files have run through the human screening\n",
    "# Do you see a *_1.fastq.gz and *_2.fastq.gz file in the working directory?\n",
    "# Of not, you will need to check your job out files above for clues about what went wrong.\n",
    "!ls $work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the fastq files (post-human filter) look smaller than the ones that were trimmed?\n",
    "# list the file sizes for fastq files post-human filtering\n",
    "!ls -l $work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993611c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the file sizes for fastq files before human filtering'\n",
    "!ls -l /xdisk/bhurwitz/bh_class/$netid/assignments/06_qc_trimming/trimmed_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b30b51",
   "metadata": {},
   "source": [
    "Great job! It looks like you have removed *most* of the human reads from your fastq files. We will double check this when we run kraken2 on the files to classify each of the reads by taxonomy.\n",
    "\n",
    "Another quick note, you may need to remove additional contamination using the same approach. For example, the sequencing center may have use PhiX as a \"spike-in\" to assess the quality of the sequencing run with a known quantity of DNA. You can use the same approach as above to remove reads from any genome you think may be contaminating your sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83501ab5",
   "metadata": {},
   "source": [
    "## Final Step\n",
    "Copy your notebook to the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp ~/07_contam_removal.ipynb $work_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
