{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de7bac8",
   "metadata": {},
   "source": [
    "# Quality Control and Trimming\n",
    "\n",
    "This notebook will go through the workflow for read quality control and trimming. We will follow each of the steps below, that will require time on the HPC to run. Be sure to check back after each step to make sure you have the right files, and start the next step.   \n",
    "\n",
    "1. Quality control using fastqc to determine quality thresholds.\n",
    "2. Compressing files before trimming.\n",
    "3. Trimming reads with [Trimmomatic](https://carpentries-lab.github.io/metagenomics-analysis/03-trimming-filtering/index.html).\n",
    "4. Optional: Final QC check after trimming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebed36",
   "metadata": {},
   "source": [
    "## Getting Started (Run this before each step)\n",
    "\n",
    "You will need to rerun this section each time you come back to this notebook to kick off the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables for your netid and xfile\n",
    "netid = \"MY_NETID\"\n",
    "xfile = \"MY_XFILE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d388ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into the working directory\n",
    "work_dir = \"/xdisk/bhurwitz/bh_class/\" + netid + \"/assignments/06_qc_trimming\"\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dffed",
   "metadata": {},
   "source": [
    "## Creating a config file\n",
    "Each of the scripts below executes code that requires certain variables to be set. So we don't need to edit the code in each of the scripts, we are going to use a config file that defines all of these variables. Then when we want to use these variables in the script, we will \"source\" the config file to set the variables. This is generally a good practice in writing scripts on the HPC, that makes it so you only need to modify the config file (rather than each individual script). We are going to create this file using the variables you set above in \"Getting started\". Note that you only need to create this config file once, even if you are returning to complete the next step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config file with all of the variables you need\n",
    "!echo \"export NETID=$netid\" > config.sh\n",
    "!echo \"export XFILE=$xfile\" >> config.sh\n",
    "!echo \"export WORK_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/06_qc_trimming\" >> config.sh\n",
    "!echo \"export FASTQ_DIR=/xdisk/bhurwitz/bh_class/$netid/assignments/05_getting_data\" >> config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the config file to be sure it is correct\n",
    "# Is your netid and xfile correct? Do you have the right directories?\n",
    "!cat config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeaabfe",
   "metadata": {},
   "source": [
    "## Step 1: Assessing Read Quality\n",
    "\n",
    "Now that we have all of our data downloaded, we are ready to start the quality control process. We will use a tool called fastqc that generates a report about the quality of our sequence data.\n",
    "\n",
    "First, we will create an sbatch script that runs fastqc on each of the sequence files. Note that when you kick off this analysis by running \"sbatch\" below, you will need to wait ~1 hour for the results to come back, depending on the queue wait time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run fastqc on each of our accessions\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via\n",
    "# the `source ./config.sh` command in the script.\n",
    "# 2. fastqc runs on each of the fastq files in the $FASTQ_DIR\n",
    "# 3. We are creating a directory called check_fastqc in our home directory\n",
    "# this allows us to copy the *html files produced by fastqc and explore\n",
    "# them using Jupyter server on the on demand hpc portal.\n",
    "# 4. The fastqc program runs in the $FASTQ_DIR, but to keep our files\n",
    "# organized, we are going to move the results into our $WORK_DIR.\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-4                         \n",
    "#SBATCH --output=Job-fastqc-%a.out\n",
    "#SBATCH --cpus-per-task=1                  \n",
    "#SBATCH --mem=4G                           \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat $FASTQ_DIR/$XFILE))\n",
    "\n",
    "apptainer run /contrib/singularity/shared/bhurwitz/fastqc-0.11.9.sif fastqc \\\n",
    "    $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq*\n",
    "\n",
    "mkdir ~/check_fastqc\n",
    "cp $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html ~/check_fastqc \n",
    "mv $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html $WORK_DIR\n",
    "mv $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.zip $WORK_DIR\n",
    " \n",
    "'''\n",
    "\n",
    "with open('fastqc_parallel.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the code and make sure your script above was created.\n",
    "!cat fastqc_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e42306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should be in your working directory when you run this script\n",
    "# do you see your config.sh file, and the fastqc_parallel.sh script?\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe884278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run sbatch to run fastqc on each of the FASTQ files\n",
    "# Remember that this may take 1 hour to run, so take a break, \n",
    "# and get a coffee.\n",
    "!sbatch ./fastqc_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "# If you get an error \"Invalid job id specified\", then your job already\n",
    "# completed.\n",
    "# You can also check for all jobs under your netid\n",
    "# !squeue --job=$netid\n",
    "!squeue --job=MY_JOBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e70ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once your jobs have run (or are running) you can check the progress\n",
    "# and also look for errors in the *out files\n",
    "# For example, you can look at Job-fastqc-0.out\n",
    "!ls\n",
    "!cat Job-fastqc-0.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that all of your files have run through fastqc.\n",
    "# Do you see a *.html and *.zip file for each one?\n",
    "!ls /xdisk/bhurwitz/bh_class/$netid/assignments/06_qc_trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b30b51",
   "metadata": {},
   "source": [
    "Great job! It looks like your files have all been checked with fastqc. Before moving on to the next step, start up the Jupyter server on HPC on demand, navigate to the folder called check_fastqc, and double click on each of the html files to check the quality of each of your sequence files. Be sure to refer back to the in-class exercise on quality control to understand what each of the sections means. Because your sequence data come from the SRA, you will likely find that your files are all passing quality control checks already. But, to be certain, we will run a few basic \"screening and cleaning\" steps via trimmomatic in Step 3 to make sure the sequences are up to par. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce6846",
   "metadata": {},
   "source": [
    "## Step 2: Compressing your *fastq files using gzip \n",
    "\n",
    "Trimmomatic works on FASTQ files that are compressed with either gzip or bzip2. So, before we can run trimmomatic, we will need to compress our read files. We'll be using gzip to compress files and get the .gz file extension we need. These FASTQ files are massive, and gzip takes time to run, so lets create a script to sbatch the compression job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a script that gzip's all of the FASTQ files\n",
    "# These are huge files, so it may take 2 hours to run.\n",
    "# This script uses gzip to compress each of the *.fastq files.\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-4              # the number of accession files\n",
    "#SBATCH --output=Job-gzip-%a.out\n",
    "#SBATCH --cpus-per-task=1        # num CPUs per task\n",
    "#SBATCH --mem=4G                 # total memory per node\n",
    " \n",
    "pwd; hostname; date\n",
    "source ./config.sh\n",
    "names=($(cat ${FASTQ_DIR}/${XFILE}))\n",
    "gzip ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq\n",
    "'''\n",
    "\n",
    "with open('gzip_untrimmedfiles.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's double check that your script was created.\n",
    "!cat gzip_untrimmedfiles.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, we are ready to kick off the script\n",
    "# Time to go get another coffee...before completing Step 3.\n",
    "!sbatch gzip_untrimmedfiles.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74429b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if it is running using the squeue command or looking for\n",
    "# jobs under your netid\n",
    "!squeue --job=MY_JOB\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if all of your *.fastq files are gzip-ed\n",
    "# Note that these files are in the \"05_getting_data\" directory\n",
    "# You should just see one gzipped file for each fastq\n",
    "# For example, ERR2198631_1.fastq.gz not ERR2198631_1.fastq too...\n",
    "# If you see both .fastq and .fastq.gz the gzip command \n",
    "# is still in progress.\n",
    "!ls /xdisk/bhurwitz/bh_class/$netid/assignments/05_getting_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eda481",
   "metadata": {},
   "source": [
    "## Step 3: Trimming .fastq Files\n",
    "\n",
    "In order to run trimmomatic in a PE (paired-end) format we'll need two files. In our case, we have *_1.fastq.gz and *_2.fastq.gz for each accession from the SRA. You should now have those from the steps above.\n",
    "\n",
    "Note that we are following the same trimming protocol from the in-class exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf10aee",
   "metadata": {},
   "source": [
    "### Initial Data Management\n",
    "The output from trimmomatic will give us 4 output files (forward paired, forward unpaired, reverse paired and reverse unpaired. To keep our data organized, let's set up some output directories so the script can organize our data as it runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_dir = work_dir + \"/trimmed_reads\"\n",
    "unpair_dir = work_dir + \"/unpaired_reads\"\n",
    "!mkdir $trim_dir\n",
    "!mkdir $unpair_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e65b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a script that runs trimmomatic on all of our fastq files\n",
    "# you can only run this after the *.fastq files are all gzip-ed (step 2)\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-4                          # the number of accessions\n",
    "#SBATCH --output=Job-trim-%a.out\n",
    "#SBATCH --cpus-per-task=1                    # num CPUs per task\n",
    "#SBATCH --mem=4G                             # total memory per node\n",
    " \n",
    "pwd; hostname; date\n",
    "source ./config.sh\n",
    "names=($(cat ${FASTQ_DIR}/${XFILE}))\n",
    "\n",
    "TRIM_DIR=\"${WORK_DIR}/trimmed_reads\"\n",
    "UNPAIR_DIR=\"${WORK_DIR}/unpaired_reads\"\n",
    "\n",
    "apptainer run /contrib/singularity/shared/bhurwitz/trimmomatic:0.39--hdfd78af_2.sif trimmomatic PE \\\n",
    "    ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz \\\n",
    "    ${TRIM_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz ${UNPAIR_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz \\\n",
    "    ${TRIM_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz ${UNPAIR_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz \\\n",
    "    ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10 SLIDINGWINDOW:4:20\n",
    "'''\n",
    "\n",
    "with open('trimmomatic_fastq.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8226ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did you create the script file correctly?\n",
    "!cat trimmomatic_fastq.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa06795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can run trimmomatic\n",
    "!sbatch trimmomatic_fastq.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f708e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "!squeue --job=MY_JOBID\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb81706",
   "metadata": {},
   "source": [
    "### Checking your output files\n",
    "\n",
    "Once your job has completed, you should see that there are four output files from two input files. The trimmomatic program places all of the \"orphaned\" reads in a separate file from the trimmed reads. Reads can become orphaned when their \"mate pair\" is either too short, or too low quality. For our analyses going forward, we will only use the reads that were trimmed, and have both the forward and reverse read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the file sizes to see that they are smaller for our trimmed\n",
    "# reads\n",
    "!echo \"trimmed:\"\n",
    "!ls -l /xdisk/bhurwitz/bh_class/$netid/assignments/06_qc_trimming/trimmed_reads/*fastq.gz\n",
    "!echo \"untrimmed:\"\n",
    "!ls -l /xdisk/bhurwitz/bh_class/$netid/assignments/05_getting_data/*fastq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545c522",
   "metadata": {},
   "source": [
    "## Step 4 (optional) QC Final Check\n",
    "\n",
    "If you have any doubts about the trimming process, you can always run fastqc on the trimmed data and double check that you see all \"green\". You can check the fastqc files using Jupyter to check for any failures or other warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5eb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are re-running fastqc on the trimmed data\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bh_class\n",
    "#SBATCH --array=0-4                         \n",
    "#SBATCH --output=Job-fastqc-trim-%a.out\n",
    "#SBATCH --cpus-per-task=1                  \n",
    "#SBATCH --mem=4G                           \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat $FASTQ_DIR/$XFILE))\n",
    "TRIM_DIR=\"${WORK_DIR}/trimmed_reads\"\n",
    "\n",
    "apptainer run /contrib/singularity/shared/bhurwitz/fastqc-0.11.9.sif fastqc \\\n",
    "    $TRIM_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq*\n",
    "\n",
    "mkdir ~/check_fastqc_trimmed\n",
    "cp $TRIM_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html ~/check_fastqc_trimmed \n",
    "\n",
    "'''\n",
    "\n",
    "with open('fastqc_trim_parallel.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch ./fastqc_trim_parallel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if your job is finished running\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd60c37",
   "metadata": {},
   "source": [
    "Once your job completes, you can look at the *.html files in your home directory in ~/check_fastqc_trimmed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83501ab5",
   "metadata": {},
   "source": [
    "## Final Step\n",
    "Copy your notebook to the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp ~/06_qc_trimming.ipynb $work_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
