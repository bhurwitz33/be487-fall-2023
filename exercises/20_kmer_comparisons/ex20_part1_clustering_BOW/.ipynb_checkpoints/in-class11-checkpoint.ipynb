{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background: clustering documents by similarity\n",
    "\n",
    "***Picture yourself reading an awesome article online about metagenomics. After finishing the article, you try to find other articles on the same topic. How can you retrieve similar articles? How do recommendation systems work?***\n",
    "\n",
    "This assignment will focus on understanding key concepts of document retrieval and clustering. Then, next in the assignment will explore how some bioinformatic tools that apply these concepts to metagenomics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Word counts and bag of words model\n",
    "\n",
    "So you want to retrieve other articles about metagenomics. But how do we do this? There are many articles out there and you don't want to go and read everything posted on the internet.\n",
    "\n",
    "So we need a way to automatically retrieve a document that might be of interest. To achieve this, first we need to decide how to measure similarity between articles? Next, we need to find a second article is similar to the one you're reading now.\n",
    "\n",
    "The bag of word model is a very popular method where we simply ignore the order of words that are present in the document. This model doesn't take into account the structure of the document (the order of the words),but instead simply counts the number of instances of every word in the document.\n",
    "\n",
    "So let's look at a specific example of this, by taking two very short documents :\n",
    "\n",
    "Document 1: *Metagenomics uses the genetic material directly from the environment.*\n",
    "\n",
    "Document 2: *We extracted the genetic material from the soil samples.*\n",
    "\n",
    "In order to assess how similar these two documents are, we can use word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the word dictionary for the document 1:\n",
    "\n",
    "document1 ={\n",
    "    \"metagenomics\": 1,\n",
    "    \"uses\": 1,\n",
    "    \"the\": 2,\n",
    "    \"genetic\":1,\n",
    "    \"material\":1,\n",
    "    \"directly\":1,\n",
    "    \"from\":1,\n",
    "    \"environment\":1\n",
    "}\n",
    "\n",
    "#create a similar dictionary for the document 2:\n",
    "\n",
    "document2 ={\n",
    "    \"we\": 1,\n",
    "    # fill the dictionary\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use these words counts to calulate a simple euclidean distance between the documents. This is easily calculated as an element-wise product over this vector.\n",
    "\n",
    "For example, the distance between the sentences \"It is a pretty cake\" and \"the cake is a lie\" is :\n",
    "\n",
    "![alt text](data/img/cake.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance for the Document 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the following documents :\n",
    "    \n",
    "Document 3: *Metagenomics uses the genetic material directly from the environment. Metagenomics uses the genetic material directly from the environment.*\n",
    "\n",
    "Document 4: *We extracted the genetic material from the samples. We extracted the genetic material from the samples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distances between document 3 and 4. What do you observe? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to correct for this bias toward long documents, we can normalize these counts :\n",
    "\n",
    "![alt text](data/img/norm_cake.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the normalized distance between document 1 and 2 and between 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background : prioritizing important words using TF-IDF distances\n",
    "\n",
    "The normalized distance that we just described helped address some of the issues with our original proposal of just using raw word counts as our representation of the document.\n",
    "\n",
    "But there's another issue, which is that we would like to emphasize the important words in a document. But, defining what is an important word is, is difficult. However, as an easy mathematical rule we could say that words like \"a\", \"the\", that are very abundant in every documents are not very important. **On the other hand, words that are abundant in one document but not in the rest of the corpus are important.**\n",
    "\n",
    "In order to take into account the importance of words in our distance calculation, we can use the TF-IDF model (“Term Frequency — Inverse Data Frequency”).\n",
    "\n",
    "* Term Frequency (tf): gives us the frequency of the word in each document in the corpus. It is the ratio of the number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases. Each document has its own tf.\n",
    "\n",
    "![alt text](data/img/TF1.png \"Title\")\n",
    "\n",
    "***with n(ij) the number of occurences of the term i in the document j\n",
    "and tf(ij) the term frequency of the term i in the document j***\n",
    "\n",
    "\n",
    "* Inverse Data Frequency (idf): is used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score. This is shown by the equation below.\n",
    "\n",
    "![alt text](data/img/IDF1.png \"Title\")\n",
    "\n",
    "***with N the number of documents \n",
    "and df(i) the number of documents containing the term i***\n",
    "\n",
    "* Combining these two we come up with the TF-IDF score (w) for a word in a document in the corpus. It is the product of tf and idf:\n",
    "\n",
    "![alt text](data/img/TF-IDF1.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our small \"cake\" example, we would get: \n",
    "\n",
    "![alt text](data/img/tf_idf_cake.png \"Title\")\n",
    "\n",
    "Now let's use python to compute automatically the tf-idf of simple examples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#Let's define the following function to calulate TF of a dictionary:\n",
    "\n",
    "def getTF(dico, bow):\n",
    "    tfDico = {}\n",
    "    bowCt = len(bow)\n",
    "    for term, count in dico.items():\n",
    "        tfDico[term] = count/float(bowCt)\n",
    "    return tfDico\n",
    "\n",
    "#Let's define a function to calculate the IDF:\n",
    "\n",
    "def getIDF(docList):\n",
    "    idfDico = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDico = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDico[word] += 1\n",
    "    \n",
    "    for word, val in idfDico.items():\n",
    "        idfDico[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDico \n",
    "\n",
    "#finally let's define the IDF-IDF function: \n",
    "\n",
    "def getTFIDF(tfBow, idfs):\n",
    "    tfidf ={}\n",
    "    for term, val in tfBow.items():\n",
    "        tfidf[term]=val*idfs[term]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate the TF-IDF for our cake sentences\n",
    "cakeA = \"it is a pretty cake\"\n",
    "cakeB = \"the cake is a lie\"\n",
    "cakeC = \"I eat my pretty cake\"\n",
    "bowA = cakeA.split(\" \")\n",
    "bowB = cakeB.split(\" \")\n",
    "bowC = cakeC.split(\" \")\n",
    "wordSet = set(set(bowA).union(set(bowB)).union(bowC))\n",
    "wordDictA = dict.fromkeys(wordSet, 0) \n",
    "wordDictB = dict.fromkeys(wordSet, 0) \n",
    "wordDictC = dict.fromkeys(wordSet, 0) \n",
    "\n",
    "for word in bowA:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in bowB:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "for word in bowC:\n",
    "    wordDictC[word]+=1\n",
    "    \n",
    "# calculate the TF\n",
    "tfBowA = getTF(wordDictA, bowA)\n",
    "tfBowB = getTF(wordDictB, bowB)\n",
    "tfBowC = getTF(wordDictC, bowC)\n",
    "#calulcate the IDF\n",
    "idfs = getIDF([wordDictA, wordDictB, wordDictC])\n",
    "#get the TF-IDF\n",
    "tfidfBowA = getTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = getTFIDF(tfBowB, idfs)\n",
    "tfidfBowC = getTFIDF(tfBowC, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "frame = pd.DataFrame([tfidfBowA, tfidfBowB, tfidfBowC])\n",
    "pd.DataFrame([tfidfBowA, tfidfBowB, tfidfBowC])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Background : Cosine distance to compute distance between documents\n",
    " \n",
    "We now have vectors of tf-idfs for the terms in each documents. We need now to compute a distance between the vectors. There are various ways to measure similarity or distances between two vectors, or in this case two documents.\n",
    "\n",
    "In this exercise we'll use a **cosine distance**. This distance is basically the cosine of the angle between the two vectors projected in a multi-dimensional space. I know. It doesn't really help.\n",
    "This distance is great because it doesn't need normalization, and can naturally handle documents of different sizes without any trouble.\n",
    "\n",
    "Although you don't really need to understand the cosine distance for this exercise, you can learn more about this in this great blog post : https://www.machinelearningplus.com/nlp/cosine-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "dataSet1 = frame.iloc[[0]].to_numpy()\n",
    "dataSet2 = frame.iloc[[1]].to_numpy()\n",
    "dataSet3 = frame.iloc[[2]].to_numpy()\n",
    "result12 = 1 - spatial.distance.cosine(dataSet1, dataSet2)\n",
    "print(result12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Use TF-IDF and cosine distance \n",
    "\n",
    "Now, it is your turn to calculate the distances between the following sentences :\n",
    "\n",
    "A = \"Metagenomics uses the genetic material directly from the environment.\"\n",
    "\n",
    "B = \"We extracted the genetic material from the soil samples.\"\n",
    "\n",
    "C = \"We love metagenomics and this awesome class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the TF-IDF for the sentences :\n",
    "sentA = \"\"#your code here\n",
    "sentB = \"\"#your code here\n",
    "sentC = \"\"#your code here\n",
    "bowA = sentA.split(\" \")\n",
    "bowB = sentB.split(\" \")\n",
    "bowC = sentC.split(\" \")\n",
    "wordSet = set(set(bowA).union(set(bowB)).union(bowC))\n",
    "wordDictA = dict.fromkeys(wordSet, 0) \n",
    "wordDictB = dict.fromkeys(wordSet, 0) \n",
    "wordDictC = dict.fromkeys(wordSet, 0) \n",
    "\n",
    "for word in bowA:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in bowB:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "for word in bowC:\n",
    "    wordDictC[word]+=1\n",
    "    \n",
    "# calculate the TF\n",
    "tfBowA = getTF(wordDictA, bowA)\n",
    "tfBowB = getTF(wordDictB, bowB)\n",
    "tfBowC = getTF(wordDictC, bowC)\n",
    "#calulcate the IDF\n",
    "idfs = getIDF([wordDictA, wordDictB, wordDictC])\n",
    "#get the TF-IDF\n",
    "tfidfBowA = getTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = getTFIDF(tfBowB, idfs)\n",
    "tfidfBowC = getTFIDF(tfBowC, idfs)\n",
    "\n",
    "frame = pd.DataFrame([tfidfBowA, tfidfBowB, tfidfBowC])\n",
    "pd.DataFrame([tfidfBowA, tfidfBowB, tfidfBowC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here use the cosine distance between the pairs of sentences\n",
    "dataSet1 = frame.iloc[[0]].to_numpy()\n",
    "dataSet2 = frame.iloc[[1]].to_numpy()\n",
    "dataSet3 = frame.iloc[[2]].to_numpy()\n",
    "result12 = 1 - spatial.distance.cosine(#your code here)\n",
    "result13 = 1 - spatial.distance.cosine(#your code here)\n",
    "result23 = 1 - spatial.distance.cosine(#your code here)\n",
    "print(#your code here)\n",
    "print(#your code here)\n",
    "print(#your code here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Clustering documents that are similar to each other\n",
    "\n",
    "We retrieved pages from biographies from real people from wikipedia and processed the text to obtain the tf-idf of each term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's load the dataset :\n",
    "pd.read_csv('data/selected_profiles_tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, the cosine distance on the tf-idf scores of 'Barack Obama' and 'Bill Clinton' is 0.8339854936884276\n",
    "on the other hand, the distance between 'Barack Obama' and 'Arnold Schwarzenegger' is 0.9457679406995915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does this result make sense? Explain briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import ward, fcluster\n",
    "\n",
    "# settings for this notebook to actually show the graphs inline\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=5, suppress=True)  # suppress scientific float notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need to import the distance matrix from the distance_sq file. This matrix contains the pair-wise cosine distance on the tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_sq= pd.read_csv('data/distance_sq.csv', header=None)\n",
    "names=['Obama','Biden','Clinton','Clooney','Pitt','Jolie','Moore','Schwarzenegger','Swift','Keys']\n",
    "dist_cond=squareform(dist_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute a hierarchical clustering using the 'ward' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(dist_cond, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=10.,  # font size for the x axis labels\n",
    "    labels=names, #add the names of the samples\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you're wondering about where the colors come from, you might want to have a look at the color_threshold argument of dendrogram(), which as not specified in our code, automagically picked a distance cut-off value of 0.7 of the final merge and then colored the first clusters below that threshold in individual colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the color_treshold=0.7 argument in the dendogram function. Then change the cut-off value of colors to \n",
    "# get clusters that make sense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
